<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>openstack对象存储：swift VS ceph | Pyinx Blog</title>

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
    <meta name="keywords" content="ceph,object storage,openstack,swift," />
  

  <meta name="description" content="&amp;nbsp;
原文地址：http://www.mirantis.com/blog/object-storage-openstack-cloud-swift-ceph/
## 
OpenStack Swift### 
### 
Swift architectureOpenStack Object Storage (Swift) provides redundant, scalable distrib">
<meta property="og:type" content="article">
<meta property="og:title" content="openstack对象存储：swift VS ceph">
<meta property="og:url" content="http://blog.itsir.org/2013/07/19/239/index.html">
<meta property="og:site_name" content="Pyinx Blog">
<meta property="og:description" content="&amp;nbsp;
原文地址：http://www.mirantis.com/blog/object-storage-openstack-cloud-swift-ceph/
## 
OpenStack Swift### 
### 
Swift architectureOpenStack Object Storage (Swift) provides redundant, scalable distrib">
<meta property="og:image" content="http://lh5.googleusercontent.com/MCFQaT6GGRK55BP590KHaY6Kcq8r1BHtLORnLmCM9gkBrJ2U_NPa4e_IRPzGvtpw7yeIZ569rjz2pbRfYUGB3V-gI-54auHeMdUmgIj5Z-9cG4HOj88">
<meta property="og:image" content="http://www.mirantis.com/wp-content/uploads/2012/10/Ceph_arch.png">
<meta property="og:updated_time" content="2015-08-06T02:38:30.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="openstack对象存储：swift VS ceph">
<meta name="twitter:description" content="&amp;nbsp;
原文地址：http://www.mirantis.com/blog/object-storage-openstack-cloud-swift-ceph/
## 
OpenStack Swift### 
### 
Swift architectureOpenStack Object Storage (Swift) provides redundant, scalable distrib">
  
  

  


  <link rel="stylesheet" href="/css/styles.css" type="text/css">
  

  
  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?57e94d016e201fba3603a8a2b0263af0";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>


</head>

<body>
  <div class="content-post">
  <a class="avatar" href="/archives">
    <img src="/images/avatar.jpg" alt="" />
  </a>
  <article id="post-239" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    <a class="post-title" href="/2013/07/19/239/">openstack对象存储：swift VS ceph</a>

  </header>

  <div class="article-meta">
    <span>2013-07-19</span>

    <span> | </span>

    <span class="article-author">pyinx</span>

    <span> | </span>

    
  <span class="article-category">
    <a class="article-category-link" href="/categories/openstack/">openstack</a>
  </span>


  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>&nbsp;</p>
<p>原文地址：<a href="http://www.mirantis.com/blog/object-storage-openstack-cloud-swift-ceph/" target="_blank" rel="external">http://www.mirantis.com/blog/object-storage-openstack-cloud-swift-ceph/</a></p>
<p>## </p>
<h2 id="OpenStack_Swift">OpenStack Swift</h2><p>### </p>
<p>### </p>
<h3 id="Swift_architecture">Swift architecture</h3><p><a href="http://www.mirantis.com/training/" target="_blank" rel="external">OpenStack Object Storage</a> (Swift) provides redundant, scalable <strong>distributed</strong> object storage using clusters of standardized servers. “Distributed” means that each piece of the data is replicated across a cluster of storage nodes. The number of replicas is configurable, but should be set to at least three for production infrastructures.</p>
<p>Objects in Swift are accessed via the REST interface, and can be stored, retrieved, and updated on demand. The object store can be easily scaled across a large number of servers.</p>
<p>Each object’s access path consists of exactly three elements:</p>
<p><em>/account/container/object</em></p>
<p>The <em>object</em> is the exact data input by the user. <em>Accounts</em> and <em>containers</em> provide a way of grouping objects. Nesting of accounts and containers is not supported.</p>
<p>Swift software is divided into components, which include <em>account servers</em>, <em>container servers</em>, and <em>object servers</em> that handle storage, replication, and management of objects, containers, and accounts. Apart from that, yet another machine called a <em>proxy server</em> exposes the swift API to users and streams objects to and from the client upon request.</p>
<p>Account servers provide container listings for a given account. Container servers provide object listings in given containers. Object servers simply return or store the object itself, given its full path.</p>
<h3 id="Rings">Rings</h3><p>Since user data is distributed over a set of machines, it is essential to keep track of where they reside. Swift does this by maintaining internal data structures called “rings.”  The rings are replicated on all the Swift cluster nodes (both storage and proxy). This way Swift avoids a flaw of many distributed filesystems that rely on a centralized metadata server, as typically, that metadata store that becomes a chokepoint for calls to reference metadata.  No update to the ring is necessary to store or remove an individual object, as rings represent cluster membership better than a central data map. This benefits IO operations, greatly reducing access latency.</p>
<p>There are separate rings for account databases, container databases, and individual objects, but each ring works in the same way. In short—for a given account, container, or object name, the ring returns information on its physical location within storage nodes. Technically, this action is carried out using the<a href="http://en.wikipedia.org/wiki/Consistent_hashing" target="_blank" rel="external">consistent hashing</a> method. Some good explanations on how the rings work can be found <a href="http://mirantis.blogspot.com/2012/02/under-hood-of-swift-ring.html" target="_blank" rel="external">on our blog</a> and<a href="http://julien.danjou.info/blog/2012/openstack-swift-consistency-analysis" target="_blank" rel="external">here</a>.</p>
<p><img src="http://lh5.googleusercontent.com/MCFQaT6GGRK55BP590KHaY6Kcq8r1BHtLORnLmCM9gkBrJ2U_NPa4e_IRPzGvtpw7yeIZ569rjz2pbRfYUGB3V-gI-54auHeMdUmgIj5Z-9cG4HOj88" alt=""></p>
<p>### </p>
<h3 id="Proxy_Server">Proxy Server</h3><p>The proxy server exposes public API and serves requests to storage entities. For each request, the proxy server looks up the location of the account, container, and object using the ring. Once it has their location, it routes the request accordingly. Objects are streamed between the proxy server and client directly and no buffering is enabled (to make it even more clear: even though it has “proxy” in the name, the “proxy” server does not do any “proxying” to speak of, such as you might be familiar with from http).</p>
<h3 id="Object_server">Object server</h3><p>This is a simple BLOB storage server that can store, retrieve, and delete objects. Objects are stored as binary files on the storage nodes with metadata stored in the file’s extended attributes (xattrs). This requires that the underlying filesystem choice for object servers support xattrs on files.</p>
<p>Each object is stored using a path derived from the object name’s hash and the operation’s time stamp. The last write always wins (including in distributed scenarios, creating a need for globally synchronized clocks) and ensures that the latest object version will be served. A deletion is also treated as a version of the file (a 0 byte file ending with ”.ts”, which stands for tombstone). This ensures that deleted files are replicated correctly and older versions don’t magically reappear due to failure scenarios.</p>
<h3 id="Container_server">Container server</h3><p>The container server handles listings of objects. It doesn’t know where those objects are, just what objects are in a specific container. The listings are stored as sqlite3 database files, and replicated across the cluster in a way similar to objects. Statistics are also tracked that include the total number of objects, and total storage usage for that container.</p>
<p>A special process—swift-container-updater—continuously sweeps the container databases on the node it works on, and updates the account database if a container’s data changes. It uses the ring to locate the account to update.</p>
<h3 id="Account_server">Account server</h3><p>This is similar to the container server, but handles container listings.</p>
<h3 id="Features_and_functions">Features and functions</h3><ul>
<li>Replication: The number of object copies that can be configured manually.</li>
<li>Object upload is a synchronous process: The proxy server returns a “201 Created” HTTP code only if more than half the replicas are written.</li>
<li>Integration with OpenStack identity service (Keystone): Accounts are mapped to tenants.</li>
<li>Auditing objects consistency: the md5 sum of an object on the file system compared to its metadata stored in xattrs.</li>
<li>Container synchronization: This makes it possible to synchronize containers across multiple data centers.</li>
<li>Handoff mechanism: It makes it possible to use an additional node to keep a replica in case of failure.</li>
<li>If the object is more than 5 Gb, it has to be split: These parts are stored as separate objects and could be read simultaneously.<br>&nbsp;</li>
</ul>
<h2 id="Ceph">Ceph</h2><p>&nbsp;</p>
<p>Ceph is a distributed network storage with distributed metadata management and POSIX semantics. The Ceph object store can be accessed with a number of clients, including a dedicated cmdline tool, FUSE, and  Amazon S3 clients (through a compatibility layer, called “<a href="http://ceph.com/wiki/S3-compatible_Gateway" target="_blank" rel="external">S3 Gateway</a>“).  Ceph is highly modular – different sets of features are provided by different components which one can mix and match. Specifically, for object store accessible via s3 API it is enough to run three of them: <strong>object server, monitori server, RADOS gateway.</strong><img src="http://www.mirantis.com/wp-content/uploads/2012/10/Ceph_arch.png" alt=""></p>
<h3 id="Monitor_server">Monitor server</h3><p><code>ceph-mon</code> is a lightweight daemon that provides a consensus for distributed decision-making in a Ceph cluster. It also is the initial point of contact for new clients, and will hand out information about the topology of the cluster. Normally there would be three <code>ceph-mon</code> daemons, on three separate physical machines, isolated from each other; for example, in different racks or rows.</p>
<h3 id="Object_server-1">Object server</h3><p>The actual data put onto Ceph is stored on top of a cluster storage engine called <a href="http://ceph.com/papers/weil-rados-pdsw07.pdf" target="_blank" rel="external">RADOS</a>, deployed on a set of storage nodes.</p>
<p><code>ceph-osd</code> is the storage daemon that runs on every storage node (object server) in the Ceph cluster.<code>ceph-osd contacts ``ceph-mon</code> for cluster membership. Its main goal is to service object read/write/etc. requests from clients, It also peers with other <code>ceph-osds</code> for data replication. The data model is fairly simple at this level. There are multiple named pools, and within each pool there are named objects, in a flat namespace (no directories). Each object has both data and metadata. The data for an object is a single, potentially big, series of bytes.  The metadata is an unordered set of key-value pairs. Ceph filesystem uses metadata to store file owner, etc. Underneath, <code>ceph-osd</code> stores the data on a local filesystem. We recommend Btrfs, but any POSIX filesystem that has extended attributes should work.</p>
<h3 id="CRUSH_algorithm">CRUSH algorithm</h3><p>While Swift uses rings (md5 hash range <strong>mapping</strong> against sets of storage nodes) for consistent data distribution and lookup, Ceph uses an <strong>algorithm</strong> called CRUSH for this.  In short, CRUSH is an algorithm that can calculate the physical location of data in Ceph, given the object name, cluster map and CRUSH rules as input. CRUSH describes the storage cluster in a hierarchy that reflects its physical organization, and thus can also ensure proper data replication on top of physical hardware. Also CRUSH allows data placement to be controlled by policy, which allows CRUSH to adapt to changes in the cluster membership.</p>
<h3 id="Rados_Gateway">Rados Gateway</h3><p>radosgw is a FastCGI service that provides a RESTful HTTP API to store objects and metadata on the Ceph cluster.</p>
<h3 id="Features_and_functions-1">Features and functions</h3><ul>
<li>Partial or complete reads and writes</li>
<li>Snapshots</li>
<li>Atomic transactions with features like append, truncate, and clone range</li>
<li>Object level key-value mappings</li>
<li>Object replicas management</li>
<li>Aggregation of objects (series of objects) into a group, and mapping the group to a series of OSDs</li>
<li>Authentication with shared secret keys: Both the client and the monitor cluster have a copy of the client’s secret key</li>
<li>Compatibility with S3/Swift API<br>&nbsp;</li>
</ul>
<h2 id="Feature_summary">Feature summary</h2><p>&nbsp;</p>
<p>&nbsp;</p>
<table border="0"><br><tbody><br><tr><br><td></td><br><td><strong>Swift</strong></td><br><td><strong>Ceph</strong></td><br></tr><br><tr><br><td>Replication</td><br><td>Yes</td><br><td>Yes</td><br></tr><br><tr><br><td>Max. obj.<br>size</td><br><td>5gb<br>(bigger objects segmented)</td><br><td>Unlimited</td><br></tr><br><tr><br><td>Multi DC<br>installation</td><br><td>Yes (replication on the container level only,<br>but a blueprint proposed for full inter dc replication)</td><br><td>No (demands asynchronous eventual consistency<br>replication, which Ceph does not yet support)</td><br></tr><br><tr><br><td>Integration<br>/w Opentsack</td><br><td>Yes</td><br><td>Partial<br>(lack of Keystone support)</td><br></tr><br><tr><br><td>Replicas<br>management</td><br><td>No</td><br><td>Yes</td><br></tr><br><tr><br><td>Writing<br>algorithm</td><br><td>Synchronous</td><br><td>Synchronous</td><br></tr><br><tr><br><td>Amazon S3<br>compatible API</td><br><td>Yes</td><br><td>Yes</td><br></tr><br><tr><br><td>Data placement<br>method</td><br><td>Ring (static mapping structure)</td><br><td>CRUSH (algorithm)</td><br></tr><br></tbody><br></table>

<p>## </p>
<h2 id="Sources">Sources</h2><p><a href="http://swift.openstack.org/overview_ring.html" target="_blank" rel="external">Official Swift documentation</a> – Source for description of data structure.<br><a href="https://github.com/openstack/swift/tree/master/swift/common/ring" target="_blank" rel="external">Swift Ring source code on Github</a> – Code base of Ring and RingBuilder Swift classes.<br><a href="http://blog.chmouel.com/" target="_blank" rel="external">Blog of Chmouel Boudjnah</a> – Contains useful Swift hints.<br><a href="http://ceph.com/docs/master/" target="_blank" rel="external">Official Ceph documentation</a> – Base source for description of data structure.</p>
<p>&nbsp;</p>
<p>&nbsp;</p>

      
    </div>

    
      

    

  </div>
</article>



<section class="disqus-comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>




</div>

  
<script>
  var disqus_shortname = 'pyinx';
  
  var disqus_url = 'http://blog.itsir.org/2013/07/19/239/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


  <!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"undefined"};
(function() {
  var ds = document.createElement('script');
  ds.type = 'text/javascript';ds.async = true;
  ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
  ds.charset = 'UTF-8';
  (document.getElementsByTagName('head')[0]
   || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script>
<!-- 多说公共JS代码 end -->

</body>
</html>
